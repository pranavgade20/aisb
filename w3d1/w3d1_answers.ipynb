{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927cf648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ViTImageProcessor {\n",
       "   \"do_convert_rgb\": null,\n",
       "   \"do_normalize\": true,\n",
       "   \"do_rescale\": true,\n",
       "   \"do_resize\": true,\n",
       "   \"image_mean\": [\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5\n",
       "   ],\n",
       "   \"image_processor_type\": \"ViTImageProcessor\",\n",
       "   \"image_std\": [\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5\n",
       "   ],\n",
       "   \"resample\": 2,\n",
       "   \"rescale_factor\": 0.00392156862745098,\n",
       "   \"size\": {\n",
       "     \"height\": 224,\n",
       "     \"width\": 224\n",
       "   }\n",
       " },\n",
       " ViTForImageClassification(\n",
       "   (vit): ViTModel(\n",
       "     (embeddings): ViTEmbeddings(\n",
       "       (patch_embeddings): ViTPatchEmbeddings(\n",
       "         (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "       )\n",
       "       (dropout): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (encoder): ViTEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x ViTLayer(\n",
       "           (attention): ViTAttention(\n",
       "             (attention): ViTSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "             )\n",
       "             (output): ViTSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): ViTIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): ViTOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "           )\n",
       "           (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "           (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "   )\n",
       "   (classifier): Linear(in_features=768, out_features=1000, bias=True)\n",
       " ),\n",
       " tensor([[[140, 144, 146,  ...,  94, 107, 102],\n",
       "          [138, 142, 139,  ..., 103, 115,  96],\n",
       "          [135, 150, 142,  ..., 103, 108,  93],\n",
       "          ...,\n",
       "          [237, 225, 236,  ..., 171, 181, 147],\n",
       "          [230, 226, 238,  ..., 114, 103,  89],\n",
       "          [238, 246, 238,  ...,  74,  74,  73]],\n",
       " \n",
       "         [[ 25,  25,  24,  ...,  16,  13,  10],\n",
       "          [ 22,  26,  20,  ...,  11,  17,  13],\n",
       "          [ 22,  33,  23,  ...,   8,  19,  10],\n",
       "          ...,\n",
       "          [100,  84,  96,  ...,  47,  62,  28],\n",
       "          [ 84,  80,  99,  ...,  24,   5,   9],\n",
       "          [100, 109,  96,  ...,  13,  25,  17]],\n",
       " \n",
       "         [[ 56,  67,  73,  ...,  38,  39,  33],\n",
       "          [ 57,  49,  48,  ...,  36,  42,  31],\n",
       "          [ 42,  59,  53,  ...,  32,  39,  26],\n",
       "          ...,\n",
       "          [190, 196, 203,  ..., 131, 144, 110],\n",
       "          [221, 213, 202,  ...,  62,  46,  44],\n",
       "          [175, 191, 214,  ...,  29,  44,  42]]], dtype=torch.uint8))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_model_and_image():\n",
    "    \"\"\"Load a pre-trained ViT model and a sample image.\"\"\"\n",
    "    # Load the model\n",
    "    processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "    model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "    # Load a sample image\n",
    "    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "    raw_image = Image.open(requests.get(url, stream=True).raw)\n",
    "    image = torch.tensor(np.array(raw_image)).permute(2, 0, 1)\n",
    "\n",
    "    return processor, model, image\n",
    "\n",
    "load_model_and_image()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9932871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(processor, model, image):\n",
    "    \"\"\"\n",
    "    Classify an image using the ViT model.\n",
    "\n",
    "    Args:\n",
    "        processor: ViT image processor\n",
    "        model: ViT classification model\n",
    "        image: Image tensor in CHW format\n",
    "\n",
    "    Returns:\n",
    "        predicted_class_idx: Index of predicted class\n",
    "        predicted_class_name: Name of predicted class\n",
    "    \"\"\"\n",
    "    # TODO: Process the image and get model predictions\n",
    "    # - Use processor to prepare inputs\n",
    "    #   - The processor takes in the image and returns a tensor with normalized pixel values that the model was trained on\n",
    "    #   - It also crops/resizes the image to the expected input size\n",
    "    # - Run the model to get logits\n",
    "    # - Find and return the predicted class index and name\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

"""
# W3D2 - Large Language Models
<!-- toc -->

## Exercise 1: Tokenization

### Exercise 1.1

Try tokenizing the following strings with the model Qwen/Qwen3-0.6B.
How many tokens do you think each will be?

    string_list = [
        "Hello world",
        "Hello, WoRlD",
        "Hello\nworld",
        "Hello\n\nworld",
    ]
"""
from transformers import AutoTokenizer


class ModelConfig:
    SUPPORTED_MODELS = [
        "meta-llama/Meta-Llama-3-8B-Instruct",
        "Qwen/Qwen3-0.6B",
        "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "google/gemma-2-2b-it",
        "openai/gpt-oss-20b",
    ]


class ModelPromptBuilder:
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.question = None

        # default to making a new assistant role section
        self.continue_final_message = False
        self.add_generation_prompt = True
        self.history = []

    def add_to_history(self, role: str, content: str):
        assert self.continue_final_message == False

        self.history.append({
            "role": role,
            "content": content
        })

    def add_partial_to_history(self, role: str, content: str):
        assert self.continue_final_message == False

        self.history.append({
            "role": role,
            "content": content
        })
        self.continue_final_message = True
        self.add_generation_prompt = False

    def make_prompt(self, tokenizer):
        if "SOLUTION":
            prompt = tokenizer.apply_chat_template(self.history,
                                                   tokenize=False,
                                                   add_generation_prompt=self.add_generation_prompt,
                                                   continue_final_message=self.continue_final_message)
            return prompt
        else:
            # todo call tokenizer.apply_chat_template
            #   - read the docs and find out how to pass the correct params
            #   - make sure you are returning templated prompt, not tokens
            return None


def load_tokenizer(model_name: str, cache_dir: str = "/tmp/cache"):
    if "SOLUTION":
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
        )
        return tokenizer
    else:
        # todo return the tokenizer
        return None


string_list = [
    "Hello world",
    "Hello, WoRlD",
    "Hello\nworld",
    "Hello\n\nworld",
]


def part1(string_list):
    for string in string_list:
        tokenizer = load_tokenizer("Qwen/Qwen3-0.6B", "/tmp/cache-tokenizer")
        tokens = []
        if "SOLUTION":
            print(f"String: {string.encode('utf-8')}")
            tokens = tokenizer.encode(string)
        else:
            # todo tokenize the string
            pass
        print(f"Tokens: {tokens}")
        print(f"Token count: {len(tokens)}")

part1(string_list)

"""
### Exercise 1.2

Try creating a full prompt with the following question, with Qwen/Qwen3-0.6B

    "What is the capital of Japan?"

Make sure to use tokenizer.apply_chat_template. This may be helpful.

    {"role": "user", "content": question}

What do you notice?
"""

def part2():
    question = "What is the capital of Japan?"
    tokenizer = load_tokenizer("Qwen/Qwen3-0.6B", "/tmp/cache-tokenizer")
    if "SOLUTION":
        history = [
            {"role": "user", "content": question},
        ]
        prompt_with_apply_chat_template = tokenizer.apply_chat_template(history, tokenize=False)
        print(f"Prompt with apply_chat_template: {prompt_with_apply_chat_template}")
    else:
        # todo return templated prompt
        pass

part2()
"""
### Exercise 1.3

Try tokenizing all of the following questions

    question_list = [
        "What is the capital of Japan?",
        "What is the best way to make $1,000,000?",
        "What is the distance between London and Edinburgh?",
    ]

with ALL of the following models:

    model_list = [
        "meta-llama/Meta-Llama-3-8B-Instruct",
        "Qwen/Qwen3-0.6B",
        "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "google/gemma-2-2b-it",
        "openai/gpt-oss-20b",
    ]

What do you notice?

"""

def part3():
    question_list = [
        "What is the capital of Japan?",
        "What is the best way to make $1,000,000?",
        "What is the distance between London and Edinburgh?",
    ]
    prompt_list = []

    for question in question_list:
        for model_name in ModelConfig.SUPPORTED_MODELS:
            tokenizer = load_tokenizer(model_name, "/tmp/cache-tokenizer")
            if "SOLUTION":
                prompt_builder = ModelPromptBuilder(model_name)
                prompt_builder.add_to_history("user", question)
                prompt = prompt_builder.make_prompt(tokenizer)

                print(f"Model: {model_name}")
                print(prompt)
                prompt_list.append((model_name, question, prompt))
            else:
                # todo make a prompt with ModelPromptBuilder
                pass

part3()
"""
### Exercise 1.4

Time to use the prompts! Encode for Qwen/Qwen3-0.6B

    {
        "role": "user",
        "content": "I'm trying to decide whether to take another bootcamp."
    }

and make sure to use continue_final_message=False, add_generation_prompt=True.

Is this a thinking model?
"""
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


class ModelConfig:
    MODEL_CONFIG_THINK_TOKENS = {
        "begin_think": "<think>",
        "end_think": "</think>",
        "generate_kwargs": {
            "temperature": 0.6,
            "top_k": 20,
            "min_p": 0.0,
            "top_p": 0.95,
        },
    }
    MODEL_CONFIG_GPT_OSS_20B = {
        "begin_think": "<|end|><|start|>assistant<|channel|>final<|message|>analysis<|message|>",
        "end_think": "<|end|><|start|>assistant<|channel|>final<|message|>",
        "generate_kwargs": {
            "temperature": 0.6,
            "top_k": 20,
            "min_p": 0.0,
            "top_p": 0.95,
        },
    }
    MODEL_CONFIG_GEMMA = {
        "fuzzy_end_think_list": ["Answer:"],
        "generate_kwargs": {
            "repetition_penalty": 1.2,
            "temperature": 0.7,
            "top_k": 20,
            "min_p": 0.0,
            "top_p": 0.95,
        },
    }
    MODEL_CONFIG_LLAMA = {
        "fuzzy_end_think_list": ["Answer:"],
        "generate_kwargs": {
            "temperature": 0.6,
            "top_k": 20,
            "min_p": 0.0,
            "top_p": 0.95,
        },
    }

    SUPPORTED_MODELS = {
        "Qwen/Qwen2.5-0.5B": MODEL_CONFIG_THINK_TOKENS,
        "Qwen/Qwen3-0.6B": MODEL_CONFIG_THINK_TOKENS,
        "Qwen/Qwen3-1.7B": MODEL_CONFIG_THINK_TOKENS,
        "Qwen/Qwen3-4B": MODEL_CONFIG_THINK_TOKENS,
        "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B": MODEL_CONFIG_THINK_TOKENS,
        "Wladastic/Mini-Think-Base-1B": MODEL_CONFIG_GEMMA,
        "google/gemma-2-2b-it": MODEL_CONFIG_GEMMA,
        "openai/gpt-oss-20b": MODEL_CONFIG_GPT_OSS_20B,
        "meta-llama/Meta-Llama-3-8B-Instruct": MODEL_CONFIG_LLAMA,
        "meta-llama/Llama-2-7b-chat-hf": MODEL_CONFIG_LLAMA,
    }

    @staticmethod
    def get(model_name: str):
        if model_name not in ModelConfig.SUPPORTED_MODELS:
            print(f"ERROR: model {model_name} not supported")
            exit(1)
        return ModelConfig.SUPPORTED_MODELS[model_name]


class ModelPromptBuilder:
    def __init__(self, model_name: str, invokes_cot: bool = True):
        self.model_name = model_name
        self.invokes_cot = invokes_cot
        self.question = None

        # default to making a new assistant role section
        self.continue_final_message = False
        self.add_generation_prompt = True
        self.history = []

    def get_model_custom_instruction(self):
        please_write_answer = "Please write the string \"Answer: \" before the final answer."

        if self.model_name == "google/gemma-2-2b-it":
            return please_write_answer
        if self.model_name == "meta-llama/Meta-Llama-3-8B-Instruct" or self.model_name == "meta-llama/Llama-2-7b-chat-hf":
            return please_write_answer

        return None

    def add_system_instruction(self, system_instruction: str):
        self.add_to_history("system", system_instruction)

    def add_user_message(self, question: str, custom_instruction: str = None):
        self.question = question
        if custom_instruction is None:
            custom_instruction = "Let's think step by step."
        model_custom_instruction = self.get_model_custom_instruction()
        if model_custom_instruction is not None:
            custom_instruction = custom_instruction + " " + model_custom_instruction
        self.add_to_history("user", f"Question: {question}\n{custom_instruction}")

    def add_to_history(self, role: str, content: str):
        assert self.continue_final_message == False

        self.history.append({
            "role": role,
            "content": content
        })

    def add_partial_to_history(self, role: str, content: str):
        assert self.continue_final_message == False

        self.history.append({
            "role": role,
            "content": content
        })
        self.continue_final_message = True
        self.add_generation_prompt = False

    def add_think_token(self):
        model_config = ModelConfig.get(self.model_name)
        if "begin_think" in model_config:
            if (self.model_name == "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"):
                self.add_partial_to_history("assistant", "<think>")
            elif (self.model_name == "openai/gpt-oss-20b"):
                self.add_partial_to_history("assistant", "analysis")
        elif "fuzzy_end_think_list" in model_config:
            pass
        else:
            print(f"ERROR: model {self.model_name} missing CoT separator config")
            exit(1)

    def make_prompt(self, tokenizer):
        if self.invokes_cot:
            self.add_think_token()
        return self._apply_chat_template(tokenizer)

    def _apply_chat_template(self, tokenizer):
        prompt = tokenizer.apply_chat_template(self.history,
                                               tokenize=False,
                                               add_generation_prompt=self.add_generation_prompt,
                                               continue_final_message=self.continue_final_message)
        return prompt


def load_tokenizer(model_name: str, cache_dir: str = "/tmp/cache"):
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        cache_dir=cache_dir,
        trust_remote_code=True,
    )

    # Set pad token if not already set
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    return tokenizer


def load_model(model_name: str, cache_dir: str = "/tmp/cache"):
    if "SOLUTION":
        tokenizer = load_tokenizer(model_name, cache_dir)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            cache_dir=cache_dir,
        )
        return (model, tokenizer)
    else:
        # todo load the tokenizer and model
        return (model, tokenizer)


def part4():
    print("=== Part 4 ===")
    model_name = "Qwen/Qwen3-0.6B"
    question = "I'm trying to decide whether to take another bootcamp."
    print(f"Model: {model_name}")
    print(f"Question: {question}")

    (model, tokenizer) = load_model(model_name, "/tmp/cache-tokenizer")
    if "SOLUTION":
        prompt_builder = ModelPromptBuilder(model_name, invokes_cot=False)
        prompt_builder.add_to_history("user", question)  # start assistant role
        prompt = prompt_builder.make_prompt(tokenizer)

        # Use tokenizer.__call__ method for proper attention mask handling
        encoded_prompt = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(model.device)

        output = model.generate(encoded_prompt.input_ids, attention_mask=encoded_prompt.attention_mask, do_sample=False,
                                max_new_tokens=1024, temperature=None, top_p=None, top_k=None)
        decoded_output = tokenizer.decode(output[0])
    else:
        # todo generate a response, update decoded_output
        decoded_output = None
    print(decoded_output)

part4()
"""
### Exercise 1.5

Encode for Qwen/Qwen3-0.6B

    {
        "role": "user",
        "content": "I'm trying to decide whether to take another bootcamp."
    }

Then generate a model response where the model completes the same section
(e.g. continue_final_message=True in tokenizer).

Note, the solution uses do_sample=False so that it is deterministic,
but you can use temperature if you like.

What happens? And why is it an infinite loop?

"""

def part5():
    print("\n=== Part 5 ===")
    model_name = "Qwen/Qwen3-0.6B"
    question = "I'm trying to decide whether to take another bootcamp."
    print(f"Model: {model_name}")
    print(f"Question: {question}")

    (model, tokenizer) = load_model(model_name, "/tmp/cache-tokenizer")
    if "SOLUTION":
        prompt_builder = ModelPromptBuilder(model_name, invokes_cot=False)
        prompt_builder.add_partial_to_history("user", question)  # complete user query
        prompt = prompt_builder.make_prompt(tokenizer)

        # Use tokenizer.__call__ method for proper attention mask handling
        encoded_prompt = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(model.device)

        output = model.generate(encoded_prompt.input_ids, attention_mask=encoded_prompt.attention_mask, do_sample=False,
                                max_new_tokens=1024, temperature=None, top_p=None, top_k=None)
        decoded_output = tokenizer.decode(output[0])
    else:
        # todo generate a response, update decoded_output
        decoded_output = None
    print(decoded_output)

part5()
"""
### Exercise 1.6

Encode these questions (add appropriate roles etc)

    question_list = [
        "What is the capital of Japan?",
        "What is the distance between London and Edinburgh?",
    ]

for both "Qwen/Qwen3-0.6B" (thinking model) and "Qwen/Qwen2.5-0.5B"
(non-thinking). With each combination, examine the output.
How much impact does chain of thought have on the output?

If you want some examples, try running

    jq . < prompt_list.jsonl

Can you fix any infinite loops by modifying the prompt?

"""

def part6():
    print("\n=== Part 6 ===")
    question_list = [
        "What is the capital of Japan?",
        "What is the distance between London and Edinburgh?",
    ]

    for model_name in ["Qwen/Qwen3-0.6B", "Qwen/Qwen2.5-0.5B"]:
        for question in question_list:
            print(f"Model: {model_name}")
            print(f"Question: {question}")

            (model, tokenizer) = load_model(model_name, "/tmp/cache-tokenizer")
            if "SOLUTION":
                prompt_builder = ModelPromptBuilder(model_name, invokes_cot=False)
                prompt_builder.add_to_history("user", question)  # start assistant role
                prompt = prompt_builder.make_prompt(tokenizer)

                # Use tokenizer.__call__ method for proper attention mask handling
                encoded_prompt = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(model.device)
                # encoded_prompt = tokenizer.encode(prompt, return_tensors="pt", padding=True, truncation=True).to(model.device)

                output = model.generate(encoded_prompt.input_ids, attention_mask=encoded_prompt.attention_mask,
                                        do_sample=False, max_new_tokens=1024, temperature=None, top_p=None, top_k=None)
                decoded_output = tokenizer.decode(output[0])
            else:
                # todo generate a response, update decoded_output
                decoded_output = None

            print(decoded_output)

part6()
# %%
"""
## Exercise 2: Prompt Injection Attacks

Now let's learn how attackers bypass safety filters using prompt injection techniques.

### Background
Prompt injection works by:
1. **Context confusion**: Making the model forget its instructions
2. **Role playing**: Getting the model to adopt a different persona
3. **Encoding tricks**: Hiding malicious intent through encoding
4. **Instruction hierarchy**: Exploiting how models prioritize instructions

### Exercise 2.1 - Basic Prompt Injection

> **Difficulty**: ðŸ”´ðŸ”´âšªâšªâšª  
> **Importance**: ðŸ”µðŸ”µðŸ”µðŸ”µðŸ”µ
> 
> You should spend up to ~15 minutes on this exercise.

- Go through https://learn.snyk.io/lesson/prompt-injection
- Go through https://learn.snyk.io/lesson/insecure-output-handling
- Finally, go to https://app.grayswan.ai/arena/challenge/proving-ground/chat and try the wave 4 challenges. Post your final score (total # of breaks) before the lunch break in discord!
"""

# %%
"""
## Exercise 3: Model Extraction Attacks

Let's implement the model extraction attack from the paper.

### Exercise 3.1 - Complete Model Dimension Extraction

> **Difficulty**: ðŸ”´ðŸ”´ðŸ”´âšªâšª  
> **Importance**: ðŸ”µðŸ”µðŸ”µðŸ”µâšª
> 
> You should spend up to ~45 minutes on this exercise.

Complete the implementation of model dimension extraction using SVD.
"""

# %%
import torch
import numpy as np
import matplotlib.pyplot as plt
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from tqdm import tqdm

# %%
# 1. Load the model and tokenizer
model_name="openai-community/gpt2"
print(f"Loading model: {model_name}...")
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)
model.eval()

def get_next_logits(input_ids):
    """
    Get the logits for the next token given input_ids.
    """
    assert input_ids.ndim == 2, "Input IDs should be a 2D tensor (batch_size, sequence_length)"
    with torch.no_grad():
        outputs = model(input_ids)
        return outputs.logits[:, -1, :]

# Set pad token if it's not set
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

if "SOLUTION":
    n_queries=1000
    max_prompt_length=10
    vocab_size = tokenizer.vocab_size
    print(f"Vocabulary size (l): {vocab_size}")
    print(f"Number of queries (n): {n_queries}")

    # 2. Initialize an empty matrix (list of logit vectors)
    logit_matrix_q = []

    # 3. Generate n random prompts and collect logits
    print(f"Querying model {n_queries} times...")
    for _ in tqdm(range(n_queries)):
        # Generate a random prompt of variable length
        prompt_length = np.random.randint(1, max_prompt_length)
        random_tokens = np.random.randint(0, vocab_size, size=prompt_length)
        input_ids = torch.tensor([random_tokens])

        # Get model outputs (logits)
        logit_matrix_q.append(get_next_logits(input_ids).numpy())

    # 4. Convert list to a NumPy matrix Q
    Q = np.vstack(logit_matrix_q)
    print(f"Shape of logit matrix Q: {Q.shape}") # Should be (n_queries, vocab_size)

    # 5. Compute the Singular Values of Q
    print("Computing Singular Value Decomposition (SVD)...")
    # We only need the singular values (S), not U and Vh
    singular_values = np.linalg.svd(Q, compute_uv=False)

    # 6. Plot the results
    print("Plotting results...")
    plt.figure(figsize=(10, 6))
    plt.plot(singular_values)
    plt.yscale('log')
    plt.title(f'Sorted Singular Values of Logit Matrix for {model_name}')
    plt.xlabel('Sorted Singular Values (Index)')
    plt.ylabel('Magnitude (log scale)')
    plt.grid(True)

    # The paper notes GPT-2 Small has a hidden dimension of 768.
    # We add a vertical line to mark this expected drop-off point.
    known_hidden_dim = 768
    plt.axvline(x=known_hidden_dim, color='r', linestyle='--', label=f'Known Hidden Dim: {known_hidden_dim}')
    plt.legend()
    plt.show()
else:
    # TODO: Implement the model dimension extraction method
    #   1. Load the model and tokenizer
    #   2. Make n_queries (1k should be enough) random queries to collect logits
    #   3. Stack logits into a matrix Q
    #   4. Compute SVD of Q
    #   5. Find the "elbow" in singular values to estimate dimension
    #   6. Plot the results
    pass
# %%
"""
### Exercise 2.3 extracting model weights

> **Difficulty**: ðŸ”´ðŸ”´ðŸ”´ðŸ”´ðŸ”´ 
> **Importance**: ðŸ”µðŸ”µâšªâšªâšª
> 
> You should spend up to ~60 minutes on this exercise.
"""

if "SOLUTION":
    vocab_size = tokenizer.vocab_size

    batch_size = 2
    # vocab_subset_indices = np.random.choice(vocab_size, 2000, replace=False)  # without this, SVD takes too much memory
    print(f"Querying model {n_queries}*{batch_size} times...")
    logit_vectors = []
    for _ in tqdm(range(n_queries)):
        prompt_length = np.random.randint(1, max_prompt_length)
        input_ids = torch.randint(0, vocab_size, (batch_size, prompt_length))
        next_token_logits = [l for l in get_next_logits(model, input_ids)]
        logit_vectors.extend(next_token_logits)

    Q = torch.stack(logit_vectors).T

    print("Computing Singular Value Decomposition (SVD)...")
    U, s, Vh = torch.linalg.svd(Q, full_matrices=False)

    log_s = torch.log(s)
    gaps = log_s[:-1] - log_s[1:]
    detected_h = 768  # known hidden dimension for GPT-2 Small, from the attack above
    print(f"Using hidden dimension (h): {detected_h}")

    U_h = U[:, :detected_h]
    Sigma_h = torch.diag(s[:detected_h])
    W_extracted = U_h @ Sigma_h
else:
    # TODO: extract the model weights into W_extracted
    #   1. Collect many logit vectors from random queries
    #   2. Build query matrix Q (vocab_size x n_samples)
    #   3. Perform SVD: Q = U @ Sigma @ V^T
    #   4. Use detected dimension to extract U_h and Sigma_h
    #   5. Compute W_extracted = U_h @ Sigma_h
    pass

# %%
# Get the ground truth weights
# The lm_head contains the final projection layer weights.
# We need to transpose it to match the (vocab_size, hidden_size) shape.
true_weights = model.lm_head.weight.detach().numpy()

# %%
def compare_weights(W_extracted, W_true):
    """
    Compares the extracted weight matrix with the ground truth matrix.

    Args:
        W_extracted (numpy.ndarray): The weights recovered from the attack (W_tilde).
        W_true (numpy.ndarray): The ground truth weights from the model.

    Returns:
        tuple: (rmse, avg_cosine_sim, percentage_similarity)
    """
    print("\n--- Comparing Extracted Weights to Ground Truth ---")

    # 1. Solve for the transformation matrix G using least squares
    # We want to find G such that W_extracted @ G â‰ˆ W_true
    print("Solving for the alignment matrix G using least squares...")
    try:
        G, residuals, rank, s = np.linalg.lstsq(W_extracted, W_true, rcond=None)
    except np.linalg.LinAlgError as e:
        print(f"Error solving least squares: {e}")
        return float('nan'), float('nan'), float('nan')

    # 2. Align the extracted weights using the solved G
    W_aligned = W_extracted @ G
    print("Alignment complete.")

    # 3. Calculate Root Mean Square Error (RMSE)
    temp = (W_aligned - W_true) ** 2
    rmse = np.sqrt(temp.mean())

    # 4. Calculate Average Cosine Similarity
    # Normalize each column vector to unit length before dot product
    norm_aligned = np.linalg.norm(W_aligned, axis=0, keepdims=True)
    norm_true = np.linalg.norm(W_true, axis=0, keepdims=True)

    # Avoid division by zero for zero-norm vectors
    # This is unlikely but good practice
    norm_aligned[norm_aligned == 0] = 1
    norm_true[norm_true == 0] = 1

    W_aligned_normalized = W_aligned / norm_aligned
    W_true_normalized = W_true / norm_true

    # Calculate cosine similarity for each column and average
    cosine_similarities = (W_aligned_normalized * W_true_normalized).sum(axis=0)
    avg_cosine_sim = (cosine_similarities.mean())

    # 5. Calculate a "Percentage Similarity" metric based on relative error
    # Frobenius norm is the square root of the sum of the absolute squares of its elements.
    relative_error = np.linalg.norm(W_aligned - W_true, 'fro') / np.linalg.norm(W_true, 'fro')
    percentage_similarity = (1 - relative_error) * 100

    return rmse, avg_cosine_sim, percentage_similarity

# 4. Compare the weights and print results
rmse, cosine_sim, percent_sim = compare_weights(W_extracted, true_weights)

print("\n--- Final Results ---")
print(f"Root Mean Square Error (RMSE): {rmse:.6f}")
print(f"Average Cosine Similarity: {cosine_sim:.6f}")
print(f"Similarity Percentage: {percent_sim:.2f}%")

print("\nInterpretation:")
print("- RMSE: Lower is better. We expect values like 0.001.")
print("- Cosine Similarity: Closer to 1.0 is better, indicating the vectors are pointing in the same direction.")
print("- Similarity Percentage: Closer to 100% is better.")

"""
### Extensions to try
- What if you only have access to topk logits instead of the full logits?
- What if you don't have access to the logits? (this is quiet expensive, so just extract one logit with the method listed in appendix F)
"""

"""
# Exercise 4 - Model Editing

> **Difficulty**: ðŸ”´âšªâšªâšªâšª
> **Importance**: ðŸ”µðŸ”µðŸ”µðŸ”µâšª
> 
> You should spend up to ~60 minutes on this exercise.

1. read https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/
2. play with and understand https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/rome.ipynb
3. pick a "fake news" and add it to the model
4. Solution: https://colab.research.google.com/drive/16RPph6SobDLhisNzA5azcP-0uMGGq10R?usp=sharing&ref=blog.mithrilsecurity.io
"""

"""
# Exercise 5 - Training models

> **Difficulty**: ðŸ”´ðŸ”´ðŸ”´ðŸ”´ðŸ”´
> **Importance**: ðŸ”µðŸ”µðŸ”µðŸ”µðŸ”µ
> 
> You should spend up to ~120 minutes on this exercise.

Ask a TA for a chat here. We want you to set up training pipeline on fireworks, and train a LORA on a model.
Then, you can take one of the following paths (for a small toy topic, like loving/hating pineapple on pizza):

- Training models erodes guardrails
- you can train them to remove guardrails "abliteration" - see https://huggingface.co/blog/mlabonne/abliteration
- you can do sneaky attacks like this aisi paper https://arxiv.org/pdf/2502.14828
"""